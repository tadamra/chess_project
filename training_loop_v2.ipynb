{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20350,"status":"ok","timestamp":1693216833101,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"c7stEpWjIzJJ","outputId":"8ba3d70c-9711-4be4-b808-b221fe297acb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/dlam_project\n","/content/drive/My Drive/dlam_project\n"]}],"source":["#potentially alternative to uploading\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/dlam_project\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqaJTALU7AiS"},"outputs":[],"source":["#from google.colab import files\n","#files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHaG1NHB5uNr"},"outputs":[],"source":["#!ls -lha kaggle.json\n","#!mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n","#!cp kaggle.json ~/.kaggle/ # move the key to the folder\n","#!pwd # checking the present working directory\n","#!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0DZiRPH8DzA"},"outputs":[],"source":["#!pip install -q kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbv45wO74J1k"},"outputs":[],"source":["#%cd /content\n","#!kaggle datasets download arevel/chess-games\n","#!unzip -qq /content/chess-games.zip\n","#%cd /content"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1693216833102,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"N_lIM0oW4J1z"},"outputs":[],"source":["#file_path = '/content/chess_games.csv'#\n","#file_path = '/content/drive/My Drive/dlam_project/chess_games.csv'\n","file_path = 'C:/Users/Nils/Downloads/archive/chess_games.csv'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8044,"status":"ok","timestamp":1693216841142,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"gS7cl5UO8qsO","outputId":"2e18483d-aba6-4997-8055-ecf88d3a938b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting chess\n","  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: chess\n","Successfully installed chess-1.10.0\n"]}],"source":["!pip install chess"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7503,"status":"ok","timestamp":1693216848640,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"-xfTbHIk4J1w"},"outputs":[],"source":["#IMPORTS\n","import numpy as np\n","import pandas as pd\n","import chess\n","import gc\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from neural_networks import CopiedPolicyChessNetwork, CopiedPolicyChessNetworkWithExtendedMoveRep\n","from datasets import Copied_Dataset, CopiedDatasetExtendedMoveRep\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1693216848640,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"AfLtklKdE3ec"},"outputs":[],"source":["# SPECIFY DEFAULT SETTINGS\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.set_default_tensor_type(torch.DoubleTensor)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94661,"status":"ok","timestamp":1693216943294,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"yS_9k2FO4J2F","outputId":"728934e3-0aa5-499d-8f38-d3bcfa908e5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(27695, 1)\n"]}],"source":["chess_data_raw = pd.read_csv(file_path, usecols=[\"AN\",\"WhiteElo\"], nrows=200000)\n","chess_data= chess_data_raw[chess_data_raw['WhiteElo'] > 2000]\n","del chess_data_raw\n","gc.collect()\n","chess_data = chess_data[['AN']]\n","chess_data = chess_data[~chess_data['AN'].str.contains('{')]\n","chess_data = chess_data[chess_data['AN'].str.len() > 20]\n","chess_data = chess_data.sample(frac=1)\n","print(chess_data.shape)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"UlwhUm3s4J11"},"outputs":[],"source":["#board = chess.Board()\n","#dic = board.piece_map()\n","#for pos, piece in dic.items():\n","#    print(pos, piece.symbol())\n","\n","#Gets the symbol P, N, B, R, Q or K for white pieces or the lower-case variants for the black pieces.\n","\n","#board layout in storage\n","# 0 r n b q k b n r\n","# 1 p p p p p p p p\n","# 2 . . . . . . . .\n","# 3 . . . . . . . .\n","# 4 . . . . . . . .\n","# 5 . . . . . . . .\n","# 6 P P P P P P P P\n","# 7 R N B Q K B N R\n","\n","# board layout in chess\n","# 8 r n b q k b n r\n","# 7 p p p p p p p p\n","# 6 . . . . . . . .\n","# 5 . . . . . . . .\n","# 4 . . . . . . . .\n","# 3 . . . . . . . .\n","# 2 P P P P P P P P\n","# 1 R N B Q K B N R\n","#   a b c d e f g h\n","\n","# pos in piece_map is letter_tonum[letter] + 8 * number"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":289,"status":"ok","timestamp":1693224350008,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"kRlBH0l94J19"},"outputs":[],"source":["N_train = 20000\n","N_test = 5000\n","N_epochs = 20\n","#data_train = Chess_Dataset(chess_data.iloc[:N_train],)\n","#data_loader_train = DataLoader(data_train, batch_size = 32, shuffle = False)\n","#\n","#data_test = Chess_Dataset(chess_data.iloc[N_train: N_train + N_test],)\n","#data_loader_test = DataLoader(data_test, batch_size = 32, shuffle = False)\n","\n","\n","#data_train = Copied_Dataset(chess_data, N_train)\n","#data_loader_train = DataLoader(data_train, batch_size = 32, shuffle = True)\n","#\n","#data_test = Copied_Dataset(chess_data, N_test)\n","#data_loader_test = DataLoader(data_test, batch_size = 32, shuffle = True)\n","\n","data_train = CopiedDatasetExtendedMoveRep(chess_data, N_train)\n","data_loader_train = DataLoader(data_train, batch_size = 32, shuffle = True)\n","\n","data_test = CopiedDatasetExtendedMoveRep(chess_data, N_test)\n","data_loader_test = DataLoader(data_test, batch_size = 32, shuffle = True)"]},{"cell_type":"markdown","metadata":{"id":"Uksid4Iq4J2D"},"source":["# Training and Eval Loop"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1693224351666,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"EpymuKJhutjD"},"outputs":[],"source":["from torch.optim import lr_scheduler"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693224352441,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"WM008SU94J2D"},"outputs":[],"source":["lr = 3e-4\n","hidden_size = 10\n","hidden_layers = 4\n","model = CopiedPolicyChessNetworkWithExtendedMoveRep(hidden_ayers = hidden_layers, hidden_size = hidden_size).to(device)\n","metric_from = nn.CrossEntropyLoss()\n","metric_to = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436284,"status":"ok","timestamp":1693224789589,"user":{"displayName":"Nils","userId":"16086198346148382741"},"user_tz":-120},"id":"GZq4yvc84J2D","outputId":"cd370f77-818e-4697-f071-932025df1399"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.061088122329125166\n","Epoch 0 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.043669542797432744\n","Evaluation Loss:  0.03246987111962498\n","Epoch 1 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.02773460089655698\n","Evaluation Loss:  0.02434080891519981\n","Epoch 2 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.022295387660351072\n","Evaluation Loss:  0.020893766680513022\n","Epoch 3 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.0197548891693958\n","Evaluation Loss:  0.018317350149146452\n","Epoch 4 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.017838905095013784\n","Evaluation Loss:  0.017423741873115097\n","Epoch 5 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.016773420778033777\n","Evaluation Loss:  0.016761294808376796\n","Epoch 6 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.015888902419151862\n","Evaluation Loss:  0.015398417672666176\n","Epoch 7 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.015106696791376271\n","Evaluation Loss:  0.0148475647468445\n","Epoch 8 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.014841500284572925\n","Evaluation Loss:  0.014518806613515668\n","Epoch 9 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.01446319422719546\n","Evaluation Loss:  0.014112679872784577\n","Epoch 10 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.014095443643983555\n","Evaluation Loss:  0.014271881568770553\n","Epoch 11 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.013860740134804076\n","Evaluation Loss:  0.013352410743756862\n","Epoch 12 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.013501680089136443\n","Evaluation Loss:  0.01317874513052872\n","Epoch 13 starting ...\n","Datapoint 0 reached, Datapoint 9600 reached, Datapoint 19200 reached, \n","Training Loss:  0.013070688002066273\n","Evaluation Loss:  0.012664176732314058\n","Epoch 14 starting ...\n","Datapoint 0 reached, "]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m loss \u001b[39m=\u001b[39m loss_from \u001b[39m+\u001b[39m loss_to\n\u001b[0;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 30\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m data_count \u001b[39m%\u001b[39m (data_loader_train\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m \u001b[39m300\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDatapoint \u001b[39m\u001b[39m{\u001b[39;00mdata_count\u001b[39m}\u001b[39;00m\u001b[39m reached\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Nils\\miniconda3\\envs\\project_dlam\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Nils\\miniconda3\\envs\\project_dlam\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n","File \u001b[1;32mc:\\Users\\Nils\\miniconda3\\envs\\project_dlam\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n","File \u001b[1;32mc:\\Users\\Nils\\miniconda3\\envs\\project_dlam\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n","File \u001b[1;32mc:\\Users\\Nils\\miniconda3\\envs\\project_dlam\\lib\\site-packages\\torch\\optim\\adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = model.to(device)\n","model.eval()\n","losses = []\n","for data, target in data_loader_test:\n","    with torch.no_grad():\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)\n","        loss_from = metric_from(output[:, 0:6, :, :], target[:, 0:6, :, :])\n","        loss_to = metric_to(output[:, 6:12, :, :], target[:, 6:12, :, :])\n","        loss = loss_from + loss_to\n","        losses.append(loss.cpu())\n","print(np.mean(losses))\n","\n","\n","for i in range(N_epochs):\n","  model.train()\n","  print(f\"Epoch {i} starting ...\")\n","  losses = []\n","  data_count = 0\n","  for data, target in data_loader_train:\n","      model.zero_grad()\n","      data = data.to(device)\n","      target = target.to(device)\n","      output = model(data)\n","      loss_from = metric_from(output[:, 0:6, :, :], target[:, 0:6, :, :])\n","      loss_to = metric_to(output[:, 6:12, :, :], target[:, 6:12, :, :])\n","      loss = loss_from + loss_to\n","      loss.backward()\n","      optimizer.step()\n","      if data_count % (data_loader_train.batch_size * 300) == 0:\n","        print(f\"Datapoint {data_count} reached\", end=\", \")\n","      data_count += data_loader_train.batch_size\n","      losses.append(loss.detach().cpu().numpy())\n","  print(\"\")\n","  print(\"Training Loss: \", np.mean(losses))\n","  model.eval()\n","\n","  losses = []\n","  for data, target in data_loader_test:\n","    with torch.no_grad():\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)\n","        loss_from = metric_from(output[:, 0:6, :, :], target[:, 0:6, :, :])\n","        loss_to = metric_to(output[:, 6:12, :, :], target[:, 6:12, :, :])\n","        loss = loss_from + loss_to\n","        losses.append(loss.cpu())\n","  # Calculate validation loss on the development dataset\n","  validation_loss = np.mean(losses)\n","  scheduler.step(validation_loss)\n","  print(\"Evaluation Loss: \", np.mean(losses))\n","  torch.save(model.state_dict(), str(i) +'_model_weights_.pth')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#model = model.to(device)\n","#model.eval()\n","#losses = []\n","#for data, target in data_loader_test:\n","#    with torch.no_grad():\n","#        data = data.to(device)\n","#        target = target.to(device)\n","#        output = model(data)\n","#        loss_from = metric_from(output[:, 0, :], target[:, 0, :])\n","#        loss_to = metric_to(output[:, 1, :], target[:, 1, :])\n","#        loss = loss_from + loss_to\n","#        losses.append(loss.cpu())\n","#print(np.mean(losses))\n","#\n","#\n","#for i in range(N_epochs):\n","#  model.train()\n","#  print(f\"Epoch {i} starting ...\")\n","#  losses = []\n","#  data_count = 0\n","#  for data, target in data_loader_train:\n","#      model.zero_grad()\n","#      data = data.to(device)\n","#      target = target.to(device)\n","#      output = model(data)\n","#      loss_from = metric_from(output[:, 0, :], target[:, 0, :])\n","#      loss_to = metric_to(output[:, 1, :], target[:, 1, :])\n","#      loss = loss_from + loss_to\n","#      loss.backward()\n","#      optimizer.step()\n","#      if data_count % (data_loader_train.batch_size * 300) == 0:\n","#        print(f\"Datapoint {data_count} reached\", end=\", \")\n","#      data_count += data_loader_train.batch_size\n","#      losses.append(loss.detach().cpu().numpy())\n","#  print(\"\")\n","#  print(\"Training Loss: \", np.mean(losses))\n","#  model.eval()\n","#\n","#  losses = []\n","#  for data, target in data_loader_test:\n","#    with torch.no_grad():\n","#        data = data.to(device)\n","#        target = target.to(device)\n","#        output = model(data)\n","#        loss_from = metric_from(output[:, 0, :], target[:, 0, :])\n","#        loss_to = metric_to(output[:, 1, :], target[:, 1, :])\n","#        loss = loss_from + loss_to\n","#        losses.append(loss.cpu())\n","#  # Calculate validation loss on the development dataset\n","#  validation_loss = np.mean(losses)\n","#  scheduler.step(validation_loss)\n","#  print(\"Evaluation Loss: \", np.mean(losses))\n","#  torch.save(model.state_dict(), str(i) +'_model_weights_.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6mDsAquWdPA"},"outputs":[],"source":["torch.save(model.state_dict(), 'model_weights_8000.pth')\n","#to load the weights again use:\n","#model = CopiedChessNetwork(hidden_layers=hidden_layers, hidden_size=hidden_size)\n","#model.load_state_dict(torch.load('model_weights_0dot15_loss.pth'))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
